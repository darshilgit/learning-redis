# Redis Caching Patterns

## ðŸŽ¯ What You'll Learn

This example demonstrates the most important caching patterns used in production systems:

- **Cache-Aside (Lazy Loading)** - Most common pattern
- **Write-Through** - Keep cache always consistent
- **Cache Stampede Prevention** - Handle high traffic scenarios
- **Multi-Level Caching** - L1/L2/L3 architecture

## ðŸš€ Run It

```bash
# Make sure Redis is running
cd ../..
make up

# Run the example
go run main.go
```

## ðŸ“Š Caching Patterns Overview

### 1. Cache-Aside (Lazy Loading)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    1. Check cache    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  App    â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  Redis  â”‚
â”‚         â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚         â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜    (miss)           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â”‚ 2. Query DB
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   DB    â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚
     â”‚ 3. Store in cache
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Redis  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Pros:** Simple, cache only what's needed
**Cons:** First request always slow, potential stale data

### 2. Write-Through

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    Write    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  App    â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  Redis  â”‚
â”‚         â”‚             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚         â”‚    Write    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚   DB    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Pros:** Cache always consistent
**Cons:** Slower writes, may cache unused data

### 3. Write-Behind (Write-Back)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    Write    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    Async    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  App    â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  Redis  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚   DB    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Pros:** Fast writes
**Cons:** Complex, risk of data loss

## ðŸ’¡ TTL Strategy Guide

| Data Type | Recommended TTL | Reasoning |
|-----------|-----------------|-----------|
| **Stock prices** | 1-5 seconds | Changes constantly |
| **User sessions** | 30 min (sliding) | Activity-based |
| **Product catalog** | 5-15 minutes | Semi-static |
| **User profiles** | 1-24 hours | Rarely changes |
| **Static content** | 1-7 days | Almost never changes |

## âš ï¸ Cache Stampede Problem

**Problem:** Cache expires â†’ 1000 requests hit DB simultaneously

```
Cache expires at T=0
  T=0.001: Request A â†’ Cache MISS â†’ Query DB
  T=0.002: Request B â†’ Cache MISS â†’ Query DB
  T=0.003: Request C â†’ Cache MISS â†’ Query DB
  ... 1000 simultaneous DB queries!
```

**Solutions:**

1. **Distributed Lock (SETNX)**
   ```go
   // Only one process fetches from DB
   if redis.SetNX("lock:key", "1", 5*time.Second) {
       data = fetchFromDB()
       redis.Set("key", data)
       redis.Del("lock:key")
   } else {
       time.Sleep(50 * time.Millisecond)
       data = redis.Get("key")
   }
   ```

2. **Probabilistic Early Expiration**
   ```go
   // Refresh before actual expiration
   if time.Now().Add(jitter) > cachedExpiry {
       refreshInBackground()
   }
   ```

3. **Background Refresh**
   ```go
   // Never let cache expire
   go func() {
       for range time.Tick(4 * time.Minute) {
           refreshCache() // TTL is 5 min
       }
   }()
   ```

## ðŸ—ï¸ Multi-Level Caching

```
Request
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ L1: In-Memory (per server)           â”‚ â† Nanoseconds
â”‚ Size: ~100MB, TTL: 10-60 seconds     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚ miss
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ L2: Redis (shared)                   â”‚ â† Milliseconds
â”‚ Size: ~10GB, TTL: 5-30 minutes       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚ miss
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ L3: Database                         â”‚ â† Tens of ms
â”‚ Source of truth                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**When to use:**
- High-traffic applications
- Read-heavy workloads
- Data that's expensive to compute

## ðŸŽ“ Interview Talking Points

### Common Questions

**Q: "How do you handle cache invalidation?"**
> "We use a combination of TTL-based expiration and explicit invalidation. For immediate consistency, we invalidate on write. For eventual consistency, we rely on TTL. We also use Pub/Sub to broadcast invalidations to all app servers."

**Q: "How do you prevent cache stampede?"**
> "We use distributed locks with SETNX to ensure only one process rebuilds the cache. We also implement probabilistic early expiration to spread out cache refreshes."

**Q: "What's your cache hit rate target?"**
> "We target 90%+ hit rate. Below 80% means we're either caching wrong data or TTLs are too short. We monitor hit/miss ratios and adjust accordingly."

**Q: "How do you size your cache?"**
> "We estimate working set size (hot data), multiply by average object size, add 30% overhead. For sessions: 1M users Ã— 1KB Ã— 20% active = ~200MB."

### Key Metrics to Monitor

- **Hit Rate:** Should be >90%
- **Miss Rate:** High miss rate = wrong data cached
- **Eviction Rate:** High = cache too small
- **Memory Usage:** Track against capacity
- **Latency:** p50, p95, p99

## ðŸ§ª Try It Yourself

### Measure Cache Performance

```bash
docker exec -it redis redis-cli

# Monitor cache operations in real-time
MONITOR

# Get memory stats
INFO memory

# Get hit/miss stats
INFO stats
# Look for: keyspace_hits, keyspace_misses
```

### Calculate Hit Rate

```bash
# Get stats
docker exec -it redis redis-cli INFO stats | grep keyspace

# Hit Rate = hits / (hits + misses) Ã— 100%
```

## ðŸ“š Next Steps

- **Need Pub/Sub for invalidation?** â†’ See [Pub/Sub example](../pubsub/)
- **Need distributed locks?** â†’ See [Distributed Lock scenario](../interview-scenarios/02-distributed-lock/)
- **Need rate limiting?** â†’ See [Rate Limiter scenario](../interview-scenarios/04-rate-limiter/)

